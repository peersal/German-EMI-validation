{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b36c54-9def-4d76-955c-fc6f931dc3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "For this script to work the wanted Folders of this website http://lrec2022.gerparcor.texttechnologylab.org/ have to be in the same directory, and .tar have to be decomprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bae6af-a79d-4084-9a15-a87e5c617e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b218c3b-f9ca-4f7d-91d1-56b68f8c25ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: Reichstag_NG_Zoll\n",
      "  File: Reichstag\n",
      "Directory: ThirdReich\n",
      "  File: ThirdReich\n",
      "Directory: Weimar_Republic\n",
      "  File: Weimar_Republic\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Loop through all items in the current directory\n",
    "for item in os.listdir(current_dir)[2:]:\n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(os.path.join(current_dir, item)):\n",
    "        print(f\"Directory: {item}\")\n",
    "        \n",
    "        # Loop through files in this directory\n",
    "        for file in os.listdir(os.path.join(current_dir, item)):\n",
    "            print(f\"  File: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1494ee20-550c-429a-9bbf-304b491fdec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decompress_gz_file(input_file, output_file):\n",
    "    with gzip.open(input_file, 'rb') as f_in:\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        # Get the root tag and some attributes to understand the structure\n",
    "        root_tag = root.tag\n",
    "        root_attrib = root.attrib\n",
    "        # Get a sample of the elements\n",
    "        elements_sample = [elem.tag for elem in root.iter()][:10]\n",
    "        (root_tag, root_attrib, elements_sample)\n",
    "    except ET.ParseError as e:\n",
    "        str(e)\n",
    "        \n",
    "        \n",
    "    # Extracting all elements and their attributes from the XMI file\n",
    "    elements_data = []\n",
    "\n",
    "    for elem in root.iter():\n",
    "        elem_data = {\n",
    "            \"tag\": elem.tag,\n",
    "            \"attributes\": elem.attrib,\n",
    "            \"text\": elem.text\n",
    "        }\n",
    "        elements_data.append(elem_data)\n",
    "        \n",
    "    elements_df = pd.DataFrame(elements_data)\n",
    "    \n",
    "    unique_tags = elements_df['tag'].unique()\n",
    "    \n",
    "    # Extract elements with the 'Sentence' tag and their attributes\n",
    "    sentence_elements = elements_df[elements_df['tag'].str.contains('Sentence')]\n",
    "   \n",
    "    # Display the attributes of a sample sentence element to understand their structure\n",
    "    sofa_elements = elements_df[elements_df['tag'].str.contains('Sofa')]\n",
    "    \n",
    "    # Extract the 'sofaString' attribute from the 'Sofa' element to get the full text\n",
    "    full_text = sofa_elements.iloc[0]['attributes'].get('sofaString', None)\n",
    "    \n",
    "    # Extract sentences from the full text using the positional data\n",
    "    extracted_sentences = []\n",
    "\n",
    "    for index, row in sentence_elements.iterrows():\n",
    "        begin = int(row['attributes']['begin'])\n",
    "        end = int(row['attributes']['end'])\n",
    "        sentence_text = full_text[begin:end]\n",
    "        extracted_sentences.append({\n",
    "            \"id\": row['attributes']['{http://www.omg.org/XMI}id'],\n",
    "            \"begin\": begin,\n",
    "            \"end\": end,\n",
    "            \"sentence\": sentence_text\n",
    "        })\n",
    "\n",
    "    # Convert to a DataFrame for better visualization\n",
    "    sentences_df = pd.DataFrame(extracted_sentences)\n",
    "    \n",
    "    \n",
    "    # Locate elements with potential timestamp metadata\n",
    "    metadata_elements = elements_df[elements_df['tag'].str.contains('DocumentMetaData|DocumentAnnotation|AnnotatorMetaData')]\n",
    "    \n",
    "   \n",
    "    for index, row in metadata_elements.iterrows():\n",
    "        attribs = row['attributes']\n",
    "        if 'timestamp' in attribs:\n",
    "            sentences_df[\"dateDay\"] = attribs.get('dateDay')\n",
    "            sentences_df[\"dateMonth\"] = attribs.get('dateMonth')\n",
    "            sentences_df[\"dateYear\"] = attribs.get('dateYear')\n",
    "            sentences_df[\"timestamp\"] = attribs.get('timestamp')\n",
    "    \n",
    "    splits =  file_path.split(\"\\\\\")\n",
    "\n",
    "    out_csv = \"clean/\" + splits[len(splits) -1 ].split(\".xmi\")[0] + \".csv\"\n",
    "\n",
    "    \n",
    "    print(f\"Safe csv at {out_csv}\")\n",
    "    sentences_df.to_csv(out_csv, index = False)\n",
    "  \n",
    "    \n",
    "    \n",
    "def process_directory(base_dir):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        #print(root)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            if file.endswith('.gz'):\n",
    "                output_file_path = os.path.splitext(file_path)[0]  # Remove the .gz extension\n",
    "                try:\n",
    "                    decompress_gz_file(file_path, output_file_path)\n",
    "                    os.remove(file_path)  # Delete the .gz file after decompressing\n",
    "                    process_file(output_file_path)  # Process the decompressed file\n",
    "                    os.remove(output_file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "            elif file.endswith('.xmi'):\n",
    "                try:\n",
    "                    process_file(file_path)  # Process the .xmi file directly\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4969ad92-0846-46ba-aead-064f5de607eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe csv at clean/71._Sitzung_28.03.1895.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "current_dir = os.getcwd()\n",
    "process_directory(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fe0f2-e35f-4a5b-9cb0-f25209edd1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the CSV files\n",
    "directory = 'clean'\n",
    "\n",
    "# Initialize an empty list to hold the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('merged_parliament_texts.csv', index=False)\n",
    "\n",
    "print(\"All CSV files have been successfully merged into 'merged_parliament_texts.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eacfc5f-0c63-45a9-a6d3-1f570ced92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"speeches_old.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
